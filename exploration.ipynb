{
 "cells": [
  {
   "cell_type": "raw",
   "id": "norwegian-harbor",
   "metadata": {},
   "source": [
    "WORKFLOW:\n",
    "    - Scrape format page\n",
    "    - Choose meta\n",
    "    - Scrape meta archtypes\n",
    "    \n",
    "    - Iterate through event pages  <-- This step requires selenium\n",
    "        - Scrape links to each event per page\n",
    "    \n",
    "        - Visit each event page and scrape\n",
    "            - links to top decks\n",
    "            - event rating\n",
    "            - event date\n",
    "            - num players\n",
    "    \n",
    "            - For each scraped deck link, scrape\n",
    "                - deck construction\n",
    "                - title\n",
    "                - author\n",
    "                - archtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-corps",
   "metadata": {},
   "source": [
    "I originally inteded to scrape the official WotC magic.wizards.com site for tournament results but they unfortunately don't make much available outside of very recent events. After some research, tournament data will be scraped from mtgtop8.com, who offer *years* worth of back data. Some individual card data could be scraped here as well, but scryfall.com offers much more robust card data via a convenient API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-argentina",
   "metadata": {},
   "source": [
    "interesting note on data gathering: https://www.dailyesports.gg/wizards-of-the-coast-mtg-frank-karsten-stop-publishing-gp-results-win-rates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demanding-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from pymongo import MongoClient\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('192.168.0.209', 27017)\n",
    "db = client['capstone_1']\n",
    "cards_coll = db['cards']\n",
    "decks_coll = db['deck_lists']\n",
    "events_coll = db['events']\n",
    "\n",
    "# Base URLs for building requests\n",
    "scryfall_api_url = 'https://api.scryfall.com/{}'  # API docs: https://scryfall.com/docs/api\n",
    "mtgtop8_url = 'https://www.mtgtop8.com/{}'\n",
    "\n",
    "# mtgtop8.com format keys for building requests\n",
    "mt8_format_keys = {\n",
    "    'vintage': 'VI',\n",
    "    'legacy': 'LE',\n",
    "    'modern': 'MO',\n",
    "    'pioneer': 'PI',\n",
    "    'historic': 'HI',\n",
    "    'standard': 'ST',\n",
    "    'commander': 'EDH',\n",
    "    'limited': 'LI',\n",
    "    'pauper': 'PAU',\n",
    "    'peasant': 'PEA',\n",
    "    'block': 'BL',\n",
    "    'extended': 'EX',\n",
    "    'highlander': 'HIGH',\n",
    "    'canadian_highlander': 'CHL'\n",
    "}\n",
    "\n",
    "def query(link, payload={}):\n",
    "    \"\"\"A requests wrapper function\"\"\"\n",
    "    response = requests.get(link, params=payload)\n",
    "    if response.status_code != 200:\n",
    "        print('WARNING', response.status_code)\n",
    "        print(response.content)\n",
    "    return response\n",
    "\n",
    "def get_card(name_str):\n",
    "    \"\"\"Returns data from Scryfall API on an individual card by name\"\"\"\n",
    "    payload = {'fuzzy': '+'.join(name_str.split())}\n",
    "    response = query(scryfall_api_url.format('cards/named'), payload)\n",
    "    return response.json()\n",
    "\n",
    "def hot_soup(url, payload={}):\n",
    "    \"\"\"Makes a steaming bowl of hot soup\"\"\"\n",
    "    response = query(url, payload)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def gather_archtypes(meta_url):\n",
    "    \"\"\"Gathering a list of archtypes among deck strategies for a meta\"\"\"\n",
    "    soup = hot_soup(meta_url)\n",
    "    archtypes = { strat: [] for strat in ['aggro', 'control', 'combo'] }\n",
    "    for strat in soup.find_all(class_='Stable')[0].find_all(rowspan=True):  # In this table, only the style type headers use 'rowspan'\n",
    "        strat_str = strat.contents[0].lower()  # Get corrosponding key for archtypes dict\n",
    "        item = strat.parent\n",
    "\n",
    "        # Gather each archtype under each strategy type\n",
    "        while len(archtypes[strat_str]) < int(strat['rowspan']) - 1:  # Rowspan == number of archtypes under this style\n",
    "            item = item.next_sibling\n",
    "            if isinstance(item, NavigableString):\n",
    "                continue\n",
    "            if item.a:  # If this sibling has a link, we know it's what we're looking for\n",
    "                text  = item.a.text\n",
    "                num_decks = int(item.contents[3].text)\n",
    "                archtypes[strat_str].append((text, num_decks))\n",
    "    \n",
    "    return archtypes\n",
    "\n",
    "def scrape_events(meta_url, collection):\n",
    "    \"\"\"Scrape event pages using selenium\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.get(meta_url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    meta_dropdown = soup.find('select', {'name': 'meta'})  # get drop down selector for meta\n",
    "    selected_meta = meta_dropdown.find('option', selected=True)  # get current meta\n",
    "    \n",
    "    def get_next(driver, class_name):\n",
    "        \"\"\"Check if the next button is still valid\"\"\"\n",
    "        try:\n",
    "            return driver.find_element_by_class_name('Nav_PN')\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f'\\nScraping event page {page}...')\n",
    "        next_btn = get_next(driver, 'Nav_PN')\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')  # make some soup\n",
    "        \n",
    "        for event in soup.find_all(class_='Stable')[2].find_all(class_='hover_tr'):  # 10 events list table\n",
    "            \"\"\"\n",
    "                This loop iterates through event table rows, pulling out an ID number,\n",
    "                the star rating and the date of the event\n",
    "            \"\"\"\n",
    "            link = event.a  # associated hyperlink\n",
    "            eid = re.search(r\"e=(\\d+)&\", link['href']).group(1)  # unique id number\n",
    "            stars = event.find(class_='O16').find_all('img')  # star rating / level\n",
    "            collection.insert_one({\n",
    "                'id': eid,\n",
    "                'name': link.text,\n",
    "                'date': event.find(class_='S10').text,\n",
    "                'level': 4 if 'bigstar' in stars[0]['src'] else len(stars),\n",
    "                'link': mtgtop8_url.format(link['href']),\n",
    "                'meta': selected_meta.text\n",
    "            })\n",
    "        \n",
    "        if next_btn:\n",
    "            next_btn.click()\n",
    "            page +=1\n",
    "            sleep(1)\n",
    "        else:\n",
    "            driver.close()\n",
    "            break\n",
    "\n",
    "def scrape_top_decks(event_url, event_id, collection):\n",
    "    \"\"\"Takes in a url to an event page and scrapes the top decks\"\"\"\n",
    "    soup = hot_soup(event_url)\n",
    "    decks_table = soup.find_all(class_='Stable')[0]\n",
    "    # num_players = int(re.search(r\"(\\d+) players -\", decks_table.text).group(1))  This fails sometimes but I haven't found the culprit\n",
    "    for sib in decks_table.next_siblings:\n",
    "        if isinstance(sib, NavigableString):\n",
    "            continue\n",
    "        if sib.a:\n",
    "            href = sib.a['href']\n",
    "            link = mtgtop8_url.format('event' + href)\n",
    "            mainboard, sideboard, archtype = scrape_decklist(link)\n",
    "            placement, title, pilot = sib.text.split('\\n')[1:-1]\n",
    "            did = re.search(r\"d=(\\d+)&\", href).group(1)  # unique id number\n",
    "            deck = {\n",
    "                'id': did,\n",
    "                'event_id': event_id,\n",
    "                'title': title,\n",
    "                'pilot': pilot,\n",
    "                'archtype': archtype,\n",
    "                'placement': placement,\n",
    "                'mainboard': mainboard,\n",
    "                'sideboard': sideboard,\n",
    "                'link': link\n",
    "            }\n",
    "            decks_coll.insert_one(deck)\n",
    "\n",
    "def scrape_decklist(deck_url):\n",
    "    \"\"\"Takes in a deck url and returns mainboard and sideboard cards with their quantities\"\"\"\n",
    "    soup = hot_soup(deck_url)\n",
    "    card_re = re.compile(r\"(\\d+)\\s(.+)\")\n",
    "    deck_table = soup.find_all(class_='Stable')[1]\n",
    "    deck_headers = deck_table.previous_sibling.previous_sibling.find_all('td')\n",
    "    archtype = deck_headers[2].text.replace('decks', '')\n",
    "    cardlist = deck_table.table.find_all('table')\n",
    "    mainboard, sideboard = [], []\n",
    "    for row in cardlist.pop().find_all('span'):\n",
    "        count, card = card_re.search(row.parent.text).groups()\n",
    "        sideboard.append((int(count), card.strip()))\n",
    "\n",
    "    for col in cardlist:\n",
    "        for row in col.find_all('span'):\n",
    "            count, card = card_re.search(row.parent.text).groups()\n",
    "            mainboard.append((int(count), card.strip()))\n",
    "\n",
    "    return mainboard, sideboard, archtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "tracked-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_coll.find_one({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "congressional-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    form = 'standard'\n",
    "    meta = 'History - All Worlds'\n",
    "    soup = hot_soup(mtgtop8_url.format('format'), {'f': mt8_format_keys[form]})\n",
    "    meta_dropdown = soup.find('select', {'name': 'meta'})  # get drop down selector for meta\n",
    "    selected_meta = meta_dropdown.find('option', selected=True)  # get current meta\n",
    "    metas = {opt.text: mtgtop8_url.format(opt['value']) for opt in meta_dropdown.find_all('option')}  # meta URLs\n",
    "    chosen_meta = metas[meta]  # a meta url will be fed into gather_archtypes() and scrape_events()\n",
    "\n",
    "    \n",
    "    \n",
    "    archtypes = gather_archtypes(chosen_meta)\n",
    "    events = scrape_events(chosen_meta, events_coll)\n",
    "    \n",
    "#     for event in events_coll.find():\n",
    "#         scrape_top_decks(event['link'], event['id'], deck)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
